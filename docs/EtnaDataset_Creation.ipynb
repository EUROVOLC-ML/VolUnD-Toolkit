{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-caution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from scipy import signal\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "precious-carter",
   "metadata": {},
   "source": [
    "tmp = sio.loadmat(r\"./data/save_2011_2012/110101/110101_EBELz.mat\") # one file a day for station\n",
    "print(tmp.keys()) # dict_keys(['__header__', '__version__', '__globals__', 'signalz_tot'])\n",
    "print(tmp['signalz_tot'].shape) # array of 8640000 values (1d*24h*60min*60sec*100Hz)\n",
    "\n",
    "tmp = sio.loadmat(r\"./data/labels_original.mat\") # file of labels, from 2011 to 2015, 1826 days (2012 bissextile)\n",
    "print(tmp.keys()) # dict_keys(['__header__', '__version__', '__globals__', 'ACTIVITY', 'TIMELINE'])\n",
    "print(tmp['ACTIVITY'].shape) # array of 262944 values, (1826d*24h*60min)/10, one every 10 minutes \n",
    "print(tmp['TIMELINE'].shape) # array of 262944 values, type Matlab datenum\n",
    "\n",
    "save_folders = [\"save_2011_2012\", \"save_2013_2014\", \"save_2015\"]\n",
    "\n",
    "files = list()\n",
    "for i in range(len(save_folders)):\n",
    "    for (dirpath, dirnames, filenames) in os.walk(r\"./data/\" + save_folders[i]):\n",
    "        files+=filenames\n",
    "        \n",
    "len(files) # 36520files/20stations=1826days, 20 files each station"
   ]
  },
  {
   "cell_type": "raw",
   "id": "rising-polymer",
   "metadata": {},
   "source": [
    "# Convert labels from matlab to python dict\n",
    "\n",
    "labels_load = sio.loadmat(r\"./data/labels_original.mat\")\n",
    "labels = labels_load['ACTIVITY'][0].tolist()\n",
    "labels_date = labels_load['TIMELINE'][0]\n",
    "date_time = list()\n",
    "days = list()\n",
    "timestamp_label_dict = dict()\n",
    "day_labels_dict = dict()\n",
    "\n",
    "for time in tqdm(range(len(labels_date))):\n",
    "    # Convert matlab datenum to python datetime\n",
    "    python_datetime = datetime.fromordinal(int(labels_date[time])) + timedelta(days=labels_date[time]%1) - timedelta(days = 366) + timedelta(minutes=10)\n",
    "    python_timestamp = round(python_datetime.timestamp()) # remove milliseconds\n",
    "    timestamp_label_dict[python_timestamp] = labels[time]\n",
    "    \n",
    "    date_time.append(datetime.fromtimestamp(python_timestamp))\n",
    "    days.append(datetime.fromtimestamp(python_timestamp).strftime(\"%y%m%d\"))\n",
    "  \n",
    "days = list(set(days))\n",
    "days.sort()\n",
    "days = days[:-1]\n",
    "n_labels_day = int(len(labels)/len(days))\n",
    "labels_day = [labels[i:i + n_labels_day] for i in range(0, len(labels), n_labels_day)]\n",
    "\n",
    "for day in tqdm(range(len(days))):\n",
    "    day_labels_dict[days[day]] = labels_day[day]\n",
    "\n",
    "data_dict = dict()\n",
    "data_dict['DATETIME_LIST'] = date_time\n",
    "data_dict['LABELS_LIST'] = labels\n",
    "data_dict['DAYS'] = days\n",
    "data_dict['DAY-LABELS'] = day_labels_dict\n",
    "torch.save(data_dict, \"./etna_dataset/labels.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-jonathan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings to create dataset filtered and subsampled and files lists of the dataset\n",
    "data_location_raw = \"./data/\"\n",
    "labels_location = \"./etna_dataset/labels.pt\"\n",
    "data_location_processed = \"./etna_dataset/data_5Hz/\"\n",
    "save_folders = [\"save_2011_2012\", \"save_2013_2014\", \"save_2015\"]\n",
    "\n",
    "stations = [\"EBELz\", \"ECANz\", \"ECBDz\", \"ECCSz\", \"ECPNz\", \"ECZMz\", \"EFIUz\", \"EMFOz\", \"EMGRz\", \"EMNRz\", \"EMPLz\", \"EPDNz\", \"EPLCz\", \"EPOZz\", \"EPZFz\", \"ESCVz\", \"ESMLz\", \"ESPCz\", \"ESVOz\", \"EZPOz\"]\n",
    "channels_name = [\"0:EBELz\", \"1:ECANz\", \"2:ECBDz\",\"3:ECCSz\", \"4:ECPNz\", \"5:ECZMz\", \"6:EFIUz\", \"7:EMFOz\", \"8:EMGRz\", \"9:EMNRz\", \"10:EMPLz\", \"11:EPDNz\", \"12:EPLCz\", \"13:EPOZz\", \"14:EPZFz\", \"15:ESCVz\", \"16:ESMLz\", \"17:ESPCz\", \"18:ESVOz\", \"19:EZPOz\"]\n",
    "channels = [2, 4, 7, 10, 15, 17]    # Channels to keep\n",
    "\n",
    "desired_file_length = 10    #minutes\n",
    "file_frequency = 100    #Hz\n",
    "subsample_frequency = 5    #Hz\n",
    "files_day = 144    # number of files in one day\n",
    "days_to_skip = 7    # number of days before and after an event\n",
    "\n",
    "# Set up Butterworth filter\n",
    "fs = file_frequency    # Sampling frequency, default=100\n",
    "lowcut = 0.01    # Low cut-off frequency of the filter\n",
    "highcut = subsample_frequency/2    # High cut-off frequency of the filter\n",
    "nyq = 0.5 * fs\n",
    "low = lowcut / nyq\n",
    "high = highcut / nyq\n",
    "sos = signal.butter(2, [low, high], analog=False, btype='bandpass', output='sos')\n",
    "\n",
    "data_location_concat = \"./etna_dataset/data_concat_5Hz/\"\n",
    "data_location_concat_percentile = \"./etna_dataset/data_concat_5Hz/percentiles/\"\n",
    "files_list_location = \"./etna_dataset/files_list/\"\n",
    "\n",
    "dataset_files_list = \"./etna_dataset/dataset/\"\n",
    "ratio = 80    # % ratio between training set and validation set"
   ]
  },
  {
   "cell_type": "raw",
   "id": "actual-dubai",
   "metadata": {},
   "source": [
    "# Create dataset filtered and subsampled from matlab to python dict\n",
    "Path(data_location_processed).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "labels_dict = torch.load(labels_location)\n",
    "n_file_hour = int(60/desired_file_length)\n",
    "subsample_factor = int(file_frequency/subsample_frequency)\n",
    "samples_file_original = int(desired_file_length*60*file_frequency)\n",
    "samples_file_desired = int(samples_file_original/subsample_factor)\n",
    "splits_day = n_file_hour*24\n",
    "n_total_file = len(labels_dict['LABELS_LIST'])\n",
    "n_labels_day = int(len(labels_dict['LABELS_LIST'])/len(labels_dict['DAYS']))\n",
    "day_folders = list()\n",
    "datetime_day = [labels_dict['DATETIME_LIST'][i:i + n_labels_day] for i in range(0, len(labels_dict['DATETIME_LIST']), n_labels_day)]\n",
    "\n",
    "for folder in tqdm(save_folders, desc=\"Folders\"):\n",
    "    for dirpath, dirnames, filenames in os.walk(data_location_raw + folder):\n",
    "        for sub_folder in dirnames:\n",
    "            day_folders.append(os.path.abspath(os.path.join(dirpath, sub_folder)))\n",
    "\n",
    "for day in tqdm(range(len(day_folders)), desc=\"Days\"):\n",
    "    if labels_dict['DAYS'][day] in day_folders[day]:\n",
    "        day_array = np.empty((splits_day, len(stations), samples_file_desired))\n",
    "        files = sorted(os.listdir(day_folders[day]))\n",
    "        for file in tqdm(files, desc=\"Station Files\"):\n",
    "            for station in range(len(stations)):\n",
    "                if stations[station] in file:\n",
    "                    tmp = sio.loadmat(os.path.join(day_folders[day], file))\n",
    "                    data = tmp['signalz_tot'].ravel()\n",
    "                    data_filtered = signal.sosfilt(sos, data)\n",
    "                    data_subsampled = data_filtered[::subsample_factor]\n",
    "                    day_split = [data_subsampled[i:i + samples_file_desired] for i in range(0, len(data_subsampled), samples_file_desired)]\n",
    "                    if(len(day_split) == splits_day):\n",
    "                        for n_file in range(splits_day):\n",
    "                            day_array[n_file, station, :] = day_split[n_file]\n",
    "\n",
    "        for i in tqdm(range(splits_day), desc=\"Save Splits\"):\n",
    "            data_dict = dict() \n",
    "            data_dict['CHANNELS_NAME'] = channels_name\n",
    "            data_dict['TIME_DESC'] = datetime_day[day][i].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            data_dict['DATA'] = torch.Tensor(day_array[i]).unsqueeze(1)\n",
    "            label = labels_dict['DAY-LABELS'][labels_dict['DAYS'][day]][i]\n",
    "            data_dict['LABEL'] = [label]\n",
    "            data_dict['TIMESTAMP'] = [datetime.timestamp(datetime_day[day][i])]\n",
    "            filename = labels_dict['DAYS'][day] + \"_\" + str(i).zfill(3) + \"_\" + str(label) + \".pt\"\n",
    "            torch.save(data_dict, os.path.join(data_location_processed, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat dataset one channel at time,\n",
    "# Calculate percentiles of each channel and max, min, mean and std excluding values outside the percentiles\n",
    "\n",
    "Path(data_location_concat).mkdir(parents=True, exist_ok=True)\n",
    "Path(data_location_concat_percentile).mkdir(parents=True, exist_ok=True)\n",
    "p_0_01, p_99_99, p_0_1, p_99_9, mean, std, max, min, mean1, std1, max1, min1 = [[None for i in range(0,20)] for i in range(12)]\n",
    "\n",
    "files = list()\n",
    "for (dirpath, dirnames, filenames) in os.walk(data_location_processed):\n",
    "    files+=filenames\n",
    "\n",
    "label = []\n",
    "for ch in tqdm(range(len(channels_name)), desc=\"Channels\"):\n",
    "    t = []\n",
    "    # Dataset concatenation\n",
    "    for file in tqdm(files, desc=\"Files\"):\n",
    "        load = torch.load(os.path.join(data_location_processed, file))\n",
    "        t.append(load['DATA'][ch][0])\n",
    "        if(ch == 0): \n",
    "            label.append(load['LABEL'][0])\n",
    "    t = torch.cat(t, 0)\n",
    "    \n",
    "    # Percentiles calculation\n",
    "    p_0_01[ch] = np.percentile(t, 0.01)\n",
    "    p_99_99[ch] = np.percentile(t, 99.99)\n",
    "    p_0_1[ch] = np.percentile(t, 0.1)\n",
    "    p_99_9[ch] = np.percentile(t, 99.9)\n",
    "    \n",
    "    # max, min, mean and std excluding values outside the percentiles\n",
    "    mask = (t >= p_0_01[ch]) & (t <= p_99_99[ch])\n",
    "    arr = t[mask]\n",
    "    mean[ch] = arr.mean().item()\n",
    "    std[ch] = arr.std().item()\n",
    "    max[ch] = arr.max().item()\n",
    "    min[ch] = arr.min().item()\n",
    "    \n",
    "    mask = (t >= p_0_1[ch]) & (t <= p_99_9[ch])\n",
    "    arr = t[mask]\n",
    "    mean1[ch] = arr.mean().item()\n",
    "    std1[ch] = arr.std().item()\n",
    "    max1[ch] = arr.max().item()\n",
    "    min1[ch] = arr.min().item()\n",
    "    \n",
    "    data_dict = dict()\n",
    "    data_dict['DATA_CONCAT'] = t\n",
    "    data_dict['LABEL_CONCAT'] = label\n",
    "    torch.save(data_dict, os.path.join(data_location_concat, \"data_concat_CH\" + str(ch) + \".pt\"))\n",
    "    \n",
    "perc_dict = dict()\n",
    "perc_dict['perc_0.01'] = p_0_01\n",
    "perc_dict['perc_99.99'] = p_99_99\n",
    "perc_dict['perc_0.1'] = p_0_1\n",
    "perc_dict['perc_99.9'] = p_99_9\n",
    "torch.save(perc_dict, os.path.join(data_location_concat_percentile, \"percentiles.pt\"))\n",
    "        \n",
    "data_dict = dict()\n",
    "data_dict['perc_0.01'] = p_0_01\n",
    "data_dict['perc_99.99'] = p_99_99\n",
    "data_dict['MEAN'] = torch.Tensor(mean)\n",
    "data_dict['STD'] = torch.Tensor(std)\n",
    "data_dict['MAX'] = torch.Tensor(max)\n",
    "data_dict['MIN'] = torch.Tensor(min)\n",
    "torch.save(data_dict, os.path.join(data_location_concat_percentile, \"mean_std_max_min__0_01_99_99.pt\"))\n",
    "\n",
    "data_dict = dict()\n",
    "data_dict['perc_0.1'] = p_0_1\n",
    "data_dict['perc_99.9'] = p_99_9\n",
    "data_dict['MEAN'] = torch.Tensor(mean1)\n",
    "data_dict['STD'] = torch.Tensor(std1)\n",
    "data_dict['MAX'] = torch.Tensor(max1)\n",
    "data_dict['MIN'] = torch.Tensor(min1)\n",
    "torch.save(data_dict, os.path.join(data_location_concat_percentile, \"mean_std_max_min__0_1_99_9.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create following files lists:\n",
    "# - Whole dataset\n",
    "# - Files with label=0\n",
    "# - Files with label=1 or 2\n",
    "# - Files in which specific channels are ON (max and min != 0)\n",
    "# - Files in which specific channels are ON and with label=0\n",
    "# - Files in which specific channels are ON and with label=1 or 2\n",
    "\n",
    "Path(files_list_location).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "files = list()\n",
    "for (dirpath, dirnames, filenames) in os.walk(data_location_processed):\n",
    "    files+=filenames\n",
    "print(\"Number of files: \" + str(len(files)))\n",
    "\n",
    "files_0 = [files[i] for i in range(len(files)) if files[i][11:12] == \"0\"]\n",
    "print(\"Number of files with label=0: \" + str(len(files_0)))\n",
    "    \n",
    "files_1_2 = [files[i] for i in range(len(files)) if files[i][11:12] != \"0\"]\n",
    "print(\"Number of files with label=1 or label=2: \" + str(len(files_1_2)))\n",
    "        \n",
    "data_dict=dict()\n",
    "data_dict['files_list'] = files\n",
    "data_dict['files_list_0'] = files_0\n",
    "data_dict['files_list_1_2'] = files_1_2\n",
    "torch.save(data_dict, os.path.join(files_list_location, \"files_lists.pt\"))\n",
    "\n",
    "files_ON = list()\n",
    "for file in tqdm(files, desc=\"Find ON\"):\n",
    "    dict_load = torch.load(data_location_processed + file)\n",
    "    file_ok = True\n",
    "    for ch in channels:\n",
    "        data_current = dict_load['DATA'][ch]\n",
    "        if np.abs(data_current).max() == 0 and np.abs(data_current).min() == 0:\n",
    "            file_ok = False\n",
    "            break\n",
    "    if file_ok:\n",
    "        files_ON.append(file)\n",
    "print(\"Number of files in which channels: \" + str(channels) + \" are ON: \" + str(len(files_ON)))\n",
    "\n",
    "files_0_ON = [files_ON[i] for i in range(len(files_ON)) if files_ON[i][11:12] == \"0\"]\n",
    "print(\"Number of files in which channels: \" + str(channels) + \" are ON and with label=0: \" + str(len(files_0_ON)))\n",
    "\n",
    "files_1_2_ON = [files_ON[i] for i in range(len(files_ON)) if files_ON[i][11:12] != \"0\"]\n",
    "print(\"Number of files in which channels: \" + str(channels) + \" are ON and with label=1 or 2: \" + str(len(files_1_2_ON)))\n",
    "\n",
    "data_dict=dict()\n",
    "data_dict['channels'] = channels\n",
    "data_dict['files_ON'] = files_ON\n",
    "data_dict['files_0_ON'] = files_0_ON\n",
    "data_dict['files_1_2_ON'] = files_1_2_ON\n",
    "torch.save(data_dict, os.path.join(files_list_location, \"files_lists_ON.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create following files lists:\n",
    "# - Files with label = 0 and x days before and after every event\n",
    "# - Files in which specific channels are ON, with label = 0 and x days before and after every event\n",
    "# - Files in which specific channels are ON, with label = 0, x days before and after every event and without values outside the percentiles\n",
    "\n",
    "Path(files_list_location).mkdir(parents=True, exist_ok=True)\n",
    "skip_x_days = days_to_skip*files_day\n",
    "\n",
    "files = list()\n",
    "for (dirpath, dirnames, filenames) in os.walk(data_location_processed):\n",
    "    files+=filenames\n",
    "print(\"Number of files: \" + str(len(files)))\n",
    "\n",
    "files_0_OK = list()\n",
    "for i in tqdm(range(len(files)), desc=\"Files \" + str(days_to_skip) + \" days before and after\"):\n",
    "    start = 0 if (i-skip_x_days < 0) else i-skip_x_days\n",
    "    stop = len(files) if (i+skip_x_days > len(files)) else i+skip_x_days\n",
    "    file_ok = True\n",
    "    for j in range(start, stop):\n",
    "        if files[j][11:12] != \"0\":\n",
    "            file_ok = False\n",
    "            break\n",
    "    if file_ok:\n",
    "        files_0_OK.append(files[i])\n",
    "print(\"Number of files with label=0 and \" + str(days_to_skip) + \" days before and after any event: \" + str(len(files_0_OK)))\n",
    "\n",
    "files_0_OK_ON = list()\n",
    "for file in tqdm(files_0_OK, desc=\"Find ON\"):\n",
    "    dict_load = torch.load(os.path.join(data_location_processed, file))\n",
    "    file_ok = True\n",
    "    for ch in channels:\n",
    "        data_current = dict_load['DATA'][ch]\n",
    "        if np.abs(data_current).max() == 0 and np.abs(data_current).min() == 0:\n",
    "            file_ok = False\n",
    "            break\n",
    "    if file_ok:\n",
    "        files_0_OK_ON.append(file)\n",
    "print(\"Number of files in which channels: \" + str(channels) + \" are ON, with label=0 and \" + str(days_to_skip) + \" days before and after any event: \" + str(len(files_0_OK_ON)))\n",
    "  \n",
    "perc = torch.load(os.path.join(data_location_concat_percentile, \"percentiles.pt\"))\n",
    "\n",
    "files_0_OK_ON_wo_percentiles = list()\n",
    "for file in tqdm(files_0_OK_ON, desc=\"Find without values outside the percentiles\"):\n",
    "    load = torch.load(os.path.join(data_location_processed, file))\n",
    "    file_ok = True\n",
    "    for ch in channels:\n",
    "        data = load['DATA'][ch]\n",
    "        if (data.min() < perc['perc_0.01'][ch] or data.max() > perc['perc_99.99'][ch]):\n",
    "            file_ok = False\n",
    "            break\n",
    "    if file_ok:\n",
    "        files_0_OK_ON_wo_percentiles.append(file)\n",
    "print(\"Number of files in which channels: \" + str(channels) + \" are ON, with label=0, \" + str(days_to_skip) + \" days before and after any event and without values outside the percentiles: \" + str(len(files_0_OK_ON_wo_percentiles)))\n",
    "\n",
    "data_dict = dict()\n",
    "data_dict['channels'] = channels\n",
    "data_dict['files_0_' + str(days_to_skip) + 'days'] = files_0_OK\n",
    "data_dict['files_0_ON_' + str(days_to_skip) + 'days'] = files_0_OK_ON\n",
    "data_dict['files_0_ON_' + str(days_to_skip) + 'days_wo_perc'] = files_0_OK_ON_wo_percentiles\n",
    "torch.save(data_dict, os.path.join(files_list_location, \"files_lists_0_ON_\" + str(days_to_skip) + \"days_wo_perc.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create files list of files excluded: x days before and after every event and with values inside the percentiles\n",
    "\n",
    "Path(files_list_location).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "load = torch.load(os.path.join(files_list_location, \"files_lists_0_ON_\" + str(days_to_skip) + \"days_wo_perc.pt\"))\n",
    "files_0_OK_ON_wo_percentiles = load['files_0_ON_' + str(days_to_skip) + 'days_wo_perc']\n",
    "\n",
    "load = torch.load(os.path.join(files_list_location, \"files_lists_ON.pt\"))\n",
    "files_0_ON = load['files_0_ON'] # ALL 0 label files when ON\n",
    "\n",
    "files_excluded = list(set(files_0_ON) - set(files_0_OK_ON_wo_percentiles))\n",
    "\n",
    "print(\"Number of files in which channels: \" + str(channels) + \" are ON, with label=0, or inside \" + str(days_to_skip) + \" days before and after any event or with values inside the percentiles: \" + str(len(files_excluded)))\n",
    "\n",
    "data_dict = dict()\n",
    "data_dict['files_0_excluded_ON'] = files_excluded\n",
    "torch.save(data_dict, os.path.join(files_list_location, \"files_0_excluded_ON.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-defensive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create files lists for train/validation and test\n",
    "\n",
    "Path(dataset_files_list).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "load = torch.load(os.path.join(files_list_location, \"files_lists_0_ON_\" + str(days_to_skip) + \"days_wo_perc.pt\"))\n",
    "files_0_OK_ON_wo_percentiles = load['files_0_ON_' + str(days_to_skip) + 'days_wo_perc']\n",
    "\n",
    "# Round number of files to multiple of files_day \n",
    "round_files = files_day * round(len(files_0_OK_ON_wo_percentiles)/files_day)\n",
    "train_val_files = files_0_OK_ON_wo_percentiles[:round_files]\n",
    "excluded_train_val_files = files_0_OK_ON_wo_percentiles[round_files:]\n",
    "\n",
    "shuffle(train_val_files)\n",
    "split = ratio/100\n",
    "split_index = math.floor(len(train_val_files) * split)\n",
    "trainingSet = train_val_files[:split_index]\n",
    "validationSet = train_val_files[split_index:]\n",
    "\n",
    "# Test set is formed by files with label=1 or 2, label=0 excluded due to conditions above,\n",
    "# label=0 excluded by rounding training and validation set\n",
    "\n",
    "files_1_2_ON = torch.load(os.path.join(files_list_location, \"files_lists_ON.pt\"))['files_1_2_ON']\n",
    "files_excluded = torch.load(os.path.join(files_list_location, \"files_0_excluded_ON.pt\"))['files_0_excluded_ON']\n",
    "testSet = sorted(files_1_2_ON + files_excluded + excluded_train_val_files)\n",
    "\n",
    "print(\"Number of files of trainingSet: \" + str(len(trainingSet)))\n",
    "print(\"Number of files of validationSet: \" + str(len(validationSet)))\n",
    "print(\"Number of files of testSet: \" + str(len(testSet)))\n",
    "\n",
    "# if testSet is too big (1year = 52560 files) split it in different lists\n",
    "files_year = files_day*365\n",
    "if len(testSet) > files_year:\n",
    "    testSet_file_list = os.path.join(dataset_files_list, \"testSet/\")\n",
    "    Path(testSet_file_list).mkdir(parents=True, exist_ok=True)\n",
    "    big_testSet = [testSet[x:x + files_year] for x in range(0, len(testSet), files_year)]\n",
    "    print(\"\\tTest set is bigger than 1 year (\" + str(files_year) + \" files) and is splitted in:\")\n",
    "    for i in range(len(big_testSet)):\n",
    "        print(\"\\t\\tNumber of files of testSet\" + str(i) + \": \" + str(len(big_testSet[i])))\n",
    "        data_dict = dict()\n",
    "        data_dict['testSet'] = big_testSet[i]\n",
    "        torch.save(data_dict, os.path.join(testSet_file_list, \"testSet\" + str(i) + \".pt\"))\n",
    "        with open(os.path.join(testSet_file_list, \"testSet\" + str(i) + \".json\"), \"w\") as write_file:\n",
    "            json.dump(data_dict, write_file)\n",
    "\n",
    "data_dict = dict()\n",
    "data_dict['trainingSet'] = sorted(trainingSet)\n",
    "torch.save(data_dict, os.path.join(dataset_files_list, \"trainingSet.pt\"))\n",
    "with open(os.path.join(dataset_files_list, \"trainingSet.json\"), \"w\") as write_file:\n",
    "    json.dump(data_dict, write_file)\n",
    "data_dict = dict()\n",
    "data_dict['validationSet'] = sorted(validationSet)\n",
    "torch.save(data_dict, os.path.join(dataset_files_list, \"validationSet.pt\"))\n",
    "with open(os.path.join(dataset_files_list, \"validationSet.json\"), \"w\") as write_file:\n",
    "    json.dump(data_dict, write_file)\n",
    "data_dict = dict()\n",
    "data_dict['testSet'] = sorted(testSet)\n",
    "torch.save(data_dict, os.path.join(dataset_files_list, \"testSet.pt\"))\n",
    "with open(os.path.join(dataset_files_list, \"testSet.json\"), \"w\") as write_file:\n",
    "    json.dump(data_dict, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and std of trainingSet\n",
    "\n",
    "Path(dataset_files_list).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mean, std = [[None for i in range(len(channels))] for i in range(2)]\n",
    "trainingSet = torch.load(dataset_files_list + \"trainingSet.pt\")['trainingSet']\n",
    "\n",
    "for ch in tqdm(range(len(channels)), desc=\"Channels\"):\n",
    "    t = []\n",
    "    for file in tqdm(trainingSet, desc=\"Files trainingSet\"):\n",
    "        load = torch.load(data_location_processed + file)\n",
    "        t.append(load['DATA'][channels[ch]][0])\n",
    "    t = torch.cat(t, 0)\n",
    "    mean[ch] = t.mean().item()\n",
    "    std[ch] = t.std().item()\n",
    "\n",
    "data_dict = dict()\n",
    "data_dict['mean'] = mean\n",
    "data_dict['std'] = std\n",
    "torch.save(data_dict, os.path.join(dataset_files_list, \"normalize_params.pt\"))\n",
    "with open(os.path.join(dataset_files_list, \"normalize_params.json\"), \"w\") as write_file:\n",
    "    json.dump(data_dict, write_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "614px",
    "left": "1413px",
    "right": "20px",
    "top": "97px",
    "width": "455px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
